{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from repvit import RepViT, RepViTBlock\n",
    "from segment_anything.modeling import  PromptEncoder, MaskDecoder, TwoWayTransformer, ImageEncoderViT\n",
    "from repvit_cfgs import repvit_m1_0_cfgs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('teacher/MedSAM_Enc.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedSAM_Lite(nn.Module):\n",
    "    def __init__(self, \n",
    "                image_encoder, \n",
    "                mask_decoder,\n",
    "                prompt_encoder\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.image_encoder = image_encoder  \n",
    "        self.mask_decoder = mask_decoder\n",
    "        self.prompt_encoder = prompt_encoder\n",
    "\n",
    "    def forward(self, image, boxes):\n",
    "        image_embedding = self.image_encoder(image) # (B, 256, 64, 64)\n",
    "        sparse_embeddings, dense_embeddings = self.prompt_encoder( \n",
    "            points=None,\n",
    "            boxes=boxes,\n",
    "            masks=None,\n",
    "        ) # get sparse_embeddings (one-point based and bbox) and z()\n",
    "        \n",
    "        low_res_masks, iou_predictions = self.mask_decoder(\n",
    "            image_embeddings=image_embedding, # (B, 256, 64, 64)\n",
    "            image_pe=self.prompt_encoder.get_dense_pe(), # (1, 256, 64, 64)\n",
    "            sparse_prompt_embeddings=sparse_embeddings, # (B, 2, 256)\n",
    "            dense_prompt_embeddings=dense_embeddings, # (B, 256, 64, 64)\n",
    "            multimask_output=False,\n",
    "          ) # (B, 1, 256, 256)\n",
    "\n",
    "        if args.distillation:\n",
    "            return image_embedding\n",
    "        return low_res_masks, iou_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10570092\n"
     ]
    }
   ],
   "source": [
    "parameters = sum(p.numel() for p in medsam_lite_model.parameters())\n",
    "print(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "med_enc = ImageEncoderViT(\n",
    "    depth=12,\n",
    "    embed_dim=768,\n",
    "    img_size=1024,\n",
    "    mlp_ratio=4,\n",
    "    norm_layer=partial(torch.nn.LayerNorm, eps=1e-6),\n",
    "    num_heads=12,\n",
    "    patch_size=16,\n",
    "    qkv_bias=True,\n",
    "    use_rel_pos=True,\n",
    "    global_attn_indexes=[2, 5, 8, 11],\n",
    "    window_size=14,\n",
    "    out_chans=256,\n",
    ")\n",
    "\n",
    "medsam_lite_prompt_encoder = PromptEncoder(\n",
    "    embed_dim=256,\n",
    "    image_embedding_size=(64, 64),\n",
    "    input_image_size=(256, 256),\n",
    "    mask_in_chans=16\n",
    ")\n",
    "\n",
    "medsam_lite_mask_decoder = MaskDecoder(\n",
    "    num_multimask_outputs=3,\n",
    "        transformer=TwoWayTransformer(\n",
    "            depth=2,\n",
    "            embedding_dim=256,\n",
    "            mlp_dim=2048,\n",
    "            num_heads=8,\n",
    "        ),\n",
    "        transformer_dim=256,\n",
    "        iou_head_depth=3,\n",
    "        iou_head_hidden_dim=256,\n",
    ")\n",
    "\n",
    "\n",
    "medsam = MedSAM_Lite(\n",
    "    image_encoder = med_enc,\n",
    "    mask_decoder = medsam_lite_mask_decoder,\n",
    "    prompt_encoder = medsam_lite_prompt_encoder\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medsam.image_encoder.load_state_dict(model, strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medsam.mask_decoder.load_state_dict(torch.load('teacher/mask_decoder.pth'),  strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medsam.prompt_encoder.load_state_dict(torch.load('teacher/prompt_encoder.pth'),  strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(medsam.state_dict(), 'teacher/medsam.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "medsam_lite_image_encoder = RepViT(\n",
    "    cfgs=repvit_m1_0_cfgs,\n",
    "    img_size=256\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "medsam_lite_model = MedSAM_Lite(\n",
    "    image_encoder = medsam_lite_image_encoder,\n",
    "    mask_decoder = medsam_lite_mask_decoder,\n",
    "    prompt_encoder = medsam_lite_prompt_encoder\n",
    ")\n",
    "\n",
    "medsam_model  = MedSAM_Lite(\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tiny_vit_sam import TinyViT\n",
    "\n",
    "medsam_lite_image_encoder = TinyViT(\n",
    "    img_size=256,\n",
    "    in_chans=3,\n",
    "    embed_dims=[\n",
    "        64, ## (64, 256, 256)\n",
    "        128, ## (128, 128, 128)\n",
    "        160, ## (160, 64, 64)\n",
    "        320 ## (320, 64, 64) \n",
    "    ],\n",
    "    depths=[2, 2, 6, 2],\n",
    "    num_heads=[2, 4, 5, 10],\n",
    "    window_sizes=[7, 7, 14, 7],\n",
    "    mlp_ratio=4.,\n",
    "    drop_rate=0.,\n",
    "    drop_path_rate=0.0,\n",
    "    use_checkpoint=False,\n",
    "    mbconv_expand_ratio=4.0,\n",
    "    local_conv_size=3,\n",
    "    layer_lr_decay=0.8\n",
    ")\n",
    "\n",
    "medsam_lite_prompt_encoder = PromptEncoder(\n",
    "    embed_dim=256,\n",
    "    image_embedding_size=(64, 64),\n",
    "    input_image_size=(256, 256),\n",
    "    mask_in_chans=16\n",
    ")\n",
    "\n",
    "medsam_lite_mask_decoder = MaskDecoder(\n",
    "    num_multimask_outputs=3,\n",
    "        transformer=TwoWayTransformer(\n",
    "            depth=2,\n",
    "            embedding_dim=256,\n",
    "            mlp_dim=2048,\n",
    "            num_heads=8,\n",
    "        ),\n",
    "        transformer_dim=256,\n",
    "        iou_head_depth=3,\n",
    "        iou_head_hidden_dim=256,\n",
    ")\n",
    "\n",
    "tinyvit_lite_model = MedSAM_Lite(\n",
    "    image_encoder = medsam_lite_image_encoder,\n",
    "    mask_decoder = medsam_lite_mask_decoder,\n",
    "    prompt_encoder = medsam_lite_prompt_encoder\n",
    ")\n",
    "\n",
    "print(f\"tinymedsam size:{sum(p.numel() for p in medsam.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 64, 64)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'out_put' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_781660/2473638871.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtensor_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_put\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'out_put' is not defined"
     ]
    }
   ],
   "source": [
    "embedding = np.load('/mnt/embeddings_npy_1024/MR_BraTS_FLAIR_BraTS-GLI-00000-000-000.npy').squeeze(0)\n",
    "print(embedding.shape)\n",
    "tensor_embedding = torch.tensor(embedding).squeeze(0)\n",
    "embeddings = torch.stack([torch.tensor(embedding), out_put])\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tiny_vit_sam import TinyViT\n",
    "\n",
    "tiny_vit = TinyViT(\n",
    "    img_size=256,\n",
    "    in_chans=3,\n",
    "    embed_dims=[\n",
    "        64, ## (64, 256, 256)\n",
    "        128, ## (128, 128, 128)\n",
    "        160, ## (160, 64, 64)\n",
    "        320 ## (320, 64, 64) \n",
    "    ],\n",
    "    depths=[2, 2, 6, 2],\n",
    "    num_heads=[2, 4, 5, 10],\n",
    "    window_sizes=[7, 7, 14, 7],\n",
    "    mlp_ratio=4.,\n",
    "    drop_rate=0.,\n",
    "    drop_path_rate=0.0,\n",
    "    use_checkpoint=False,\n",
    "    mbconv_expand_ratio=4.0,\n",
    "    local_conv_size=3,\n",
    "    layer_lr_decay=0.8)\n",
    "\n",
    "tiny_vit.eval()\n",
    "input_image = torch.randn(1, 3, 256, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "img = np.load('/cvpr-data/train_npy_256/imgs/PET_Lesion_PETCT_0beb67c923-019.npy')\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "input_tensor = torch.randn(1, 3, 16, 16)\n",
    "\n",
    "upsample_layer = nn.Sequential(\n",
    "    nn.ConvTranspose2d(in_channels= 3, out_channels= 3, kernel_size= 4, stride=4 )\n",
    ")\n",
    "output_tensor = upsample_layer(input_tensor)\n",
    "print(output_tensor.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
